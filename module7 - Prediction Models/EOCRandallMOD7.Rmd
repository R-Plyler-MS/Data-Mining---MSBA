---
title: "EOCRandallPlylerMSBA"
author: "Randall Plyler"
date: "2/20/2022"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
library(klaR)
library(rpivotTable)
library(caret)
library(rpart)
library(PRP)
library(forecast)

```

Automobile Accidents. The file Accidents.csv contains information on 42,183 actual automobile accidents in 2001 in the United States that involved one of three levels ofinjury: NO INJURY, INJURY, or FATALITY. For each accident, additional information is recorded, such as day of week, weather conditions, and road type. A firm might be interested in developing a system for quickly classifying the severity of an
accident based on initial reports and associated data in the system (some of which rely on GPS-assisted reporting).

Our goal here is to predict whether an accident just reported will involve an injury
(MAX_SEV_IR = 1 or 2) or will not (MAX_SEV_IR = 0). For this purpose, create a dummy variable called INJURY that takes the value “yes” if MAX_SEV_IR = 1 or 2, and otherwise “no.”

a. Using the information in this dataset, if an accident has just been reported and no
further information is available, what should the prediction be? (INJURY = Yes
or No?) Why?

```{r}

accidents.df <- read.csv("C:/Users/randa/Dropbox/Masters/Winter/TBANLT 560 Data Mining/Files/AccidentsFull.csv")
#accidents.df
accidents.df$INJURY <- ifelse(accidents.df$MAX_SEV_IR>0, "yes", "no")
inj.tbl <- table(accidents.df$INJURY)
inj.tbl
inj.tbl['yes']/(inj.tbl['yes']+inj.tbl['no'])
```


b. Select the first 12 records in the dataset and look only at the response (INJURY)
and the two predictors WEATHER_R and TRAF_CON_R.

```{r}
new.df <- accidents.df[1:12,c("INJURY","WEATHER_R","TRAF_CON_R")]
new.df


```

i. Create a pivot table that examines INJURY as a function of the two predictors
for these 12 records. Use all three variables in the pivot table as rows/columns.

```{r}


A<- rpivotTable(new.df, rows="Injury", cols=c("WEATHER_R","TRAF_CON_R"), width="100%", height="400px")
A
```


ii. Compute the exact Bayes conditional probabilities of an injury (INJURY =
Yes) given the six possible combinations of the predictors.

```{r}

P_InjY_A_Numerator <- 2/3 * 3/12
P_InjY_A_Denominator <- 3/12
prob1 <- P_InjY_A_Numerator/P_InjY_A_Denominator


P_InjY_B_Numerator <- 0 * 3/12
P_InjY_B_Denominator <- 1/12
prob2 <- P_InjY_B_Numerator/P_InjY_B_Denominator


P_InjY_C_Numerator <- 0 * 3/12
P_InjY_C_Denominator <- 1/12
prob3 <- P_InjY_C_Numerator/P_InjY_C_Denominator


P_InjY_D_Numerator <- 1/3 * 3/12
P_InjY_D_Denominator <- 6/12
prob4 <- P_InjY_D_Numerator/P_InjY_D_Denominator


P_InjY_E_Numerator <- 0 * 3/12
P_InjY_E_Denominator <- 1/12
prob5 <- P_InjY_E_Numerator/P_InjY_E_Denominator


P_InjY_F_Numerator <- 0 * 3/12
P_InjY_F_Denominator <- 0
prob6 <- P_InjY_F_Numerator/P_InjY_F_Denominator

X<-c('A','B','C','D','E','F')
Y<-c(prob1,prob2,prob3,prob4,prob5,prob6)
prob.df<-data.frame(X,Y)
prob.df
```

iii. Classify the 12 accidents using these probabilities and a cutoff of 0.5.

```{r}
new.df.prob<-new.df
head(new.df.prob,12)
prob.inj <- c(0.667, 0.167, 0, 0, 0.667, 0.167, 0.167, 0.667, 0.167, 0.167, 0.167, 0)
new.df.prob$PROB_INJURY<-prob.inj
#add a column for injury prediction based on a cutoff of 0.5
new.df.prob$PREDICT_PROB<-ifelse(new.df.prob$PROB_INJURY>.5,"yes","no")
new.df.prob
```


iv. Compute manually the naive Bayes conditional probability of an injury given
WEATHER_R = 1 and TRAF_CON_R = 1.

```{r}
man.prob <- 2/3 * 0/3 * 3/12
man.prob
```

v. Run a naive Bayes classifier on the 12 records and two predictors using R.
Check the model output to obtain probabilities and classifications for all 12
records. Compare this to the exact Bayes classification. Are the resulting classifications equivalent? Is the ranking (= ordering) of observations equivalent?

```{r}
nb<-naiveBayes(INJURY ~ ., data = new.df)
predict(nb, newdata = new.df,type = "raw")
x=new.df[,-3]
y=new.df$INJURY
model <- train(x,y,'nb', trControl = trainControl(method = 'cv',number=10))
model.pred<-predict(model$finalModel,x)
model.pred
table(model.pred$class,y)
new.df.prob$PREDICT_PROB_NB<-model.pred$class
new.df.prob
```


c. Let us now return to the entire dataset. Partition the data into training (60%) and
validation (40%).

```{r}
set.seed(22)
train.index <- sample(c(1:dim(accidents.df)[1]), dim(accidents.df)[1]*0.6)  
train.df <- accidents.df[train.index,]
valid.df <- accidents.df[-train.index,]
```

i. Assuming that no information or initial reports about the accident itself are
available at the time of prediction (only location characteristics, weather conditions, etc.), which predictors can we include in the analysis? (Use the
Data_Codes sheet.)

ii. Run a naive Bayes classifier on the complete training set with the relevant predictors (and INJURY as the response). Note that all predictors are categorical.
Show the confusion matrix.

```{r}
#define which variable you will be using
vars <- c("INJURY", "ï..HOUR_I_R",  "ALIGN_I" ,"WRK_ZONE",  "WKDY_I_R",
          "INT_HWY",  "LGTCON_I_R", "PROFIL_I_R", "SPD_LIM", "SUR_COND",
          "TRAF_CON_R",   "TRAF_WAY",   "WEATHER_R")

nbTotal <- naiveBayes(INJURY ~ ., data = train.df[,vars])
nbTotal
```


iii. What is the overall error for the validation set?


```{r}
ver=1-.5354
verp=scales::percent(ver,0.01)
paste("Overall Error: ",verp)
```

iv. What is the percent improvement relative to the naive rule (using the validation
set)?

```{r}
ner=.5087
imp=ver-ner
paste("The percent improvement is ",scales::percent(imp,0.01))
```


v. Examine the conditional probabilities output. Why do we get a probability of
zero for P(INJURY = No | SPD_LIM = 5)?

```{r}
options(digits = 2)
nbTotal
```



Predicting Prices of Used Cars (Regression Trees). The file ToyotaCorolla.csv
contains the data on used cars (Toyota Corolla) on sale during late summer of 2004
in the Netherlands. It has 1436 records containing details on 38 attributes, including
Price, Age, Kilometers, HP, and other specifications. The goal is to predict the price
of a used Toyota Corolla based on its specifications. (The example in Section 9.7 is a
subset of this dataset).
Data Preprocessing. Split the data into training (60%), and validation (40%) datasets.

a. Run a regression tree (RT) with outcome variable Price and predictors Age_08_04,
KM, Fuel_Type, HP, Automatic, Doors, Quarterly_Tax, Mfg_Guarantee,
Guarantee_Period, Airco, Automatic_Airco, CD_Player, Powered_Windows,
Sport_Model, and Tow_Bar. Keep the minimum number of records in a terminal
node to 1, maximum number of tree levels to 100, and cp = 0.001, to make the
run least restrictive.

```{r}
car.df <- read.csv("C:/Users/randa/Dropbox/Masters/Winter/TBANLT 560 Data Mining/module7/ToyotaCorolla.csv")
set.seed(22)  
train.index <- sample(c(1:dim(car.df)[1]), dim(car.df)[1]*0.6)  
train.df <- car.df[train.index, ]
valid.df <- car.df[-train.index, ]
```


i. Which appear to be the three or four most important car specifications for
predicting the car’s price?

```{r}

library(rpart.plot)
TrainDF_A <- rpart(Price ~  Age_08_04 + KM + Fuel_Type + HP + Automatic + Doors + Quarterly_Tax +Mfr_Guarantee + Guarantee_Period + Airco + Automatic_airco + CD_Player + Powered_Windows + Sport_Model + Tow_Bar,data = train.df,method = "anova", minbucket = 1, maxdepth = 30, cp = 0.001)
rpart.plot(TrainDF_A)
t(t(TrainDF_A$variable.importance))
```
Age, Automatic Airconditioning, KM, and Quarterly Tax are the best predictors


ii. Compare the prediction errors of the training and validation sets by examining
their RMS error and by plotting the two boxplots. What is happening with the
training set predictions? How does the predictive performance of the validation
set compare to the training set? Why does this occur?

```{r}
accuracy(predict(TrainDF_A, train.df), train.df$Price)
accuracy(predict(TrainDF_A, valid.df), valid.df$Price)
train.err <-predict(TrainDF_A, train.df) -train.df$Price
valid.err <-predict(TrainDF_A, valid.df) -valid.df$Price
err <-data.frame(Error =c(train.err, valid.err),Set =c(rep("Training", length(train.err)),rep("Validation", length(valid.err))))
boxplot(Error~Set, data=err, main="RMS Errors",xlab = "Set", ylab = "Error",col="blueviolet",medcol="darkgoldenrod1",boxlty=0,border="black",whisklty=1,staplelwd=4,outpch=13,outcex=1,outcol="darkslateblue")

```
The validation and training sets both are relatively even which indicates the data is large enough of a sample size. 

iii. How can we achieve predictions for the training set that are not equal to the
actual prices?

By reducing the sample set and comparing the training set to the validation

iv. Prune the full tree using the cross-validation error. Compared to the full tree,
what is the predictive performance for the validation set?

```{r}
TrainDF_A.shallow <- rpart(Price ~  Age_08_04 + KM + Fuel_Type + HP + Automatic + Doors + Quarterly_Tax + Mfr_Guarantee + Guarantee_Period + Airco +Automatic_airco + CD_Player + Powered_Windows +Sport_Model + Tow_Bar, data = train.df)
rpart.plot(TrainDF_A.shallow)
```
```{r}
accuracy(predict(TrainDF_A.shallow, train.df), train.df$Price)
accuracy(predict(TrainDF_A.shallow, valid.df), valid.df$Price)
```


Pruning the tree created a worse RMSE resulting in a RMSE increase of ~100.


b. Let us see the effect of turning the price variable into a categorical variable. First,
create a new variable that categorizes price into 20 bins. Now repartition the data
keeping Binned_Price instead of Price. Run a classification tree with the same set
of input variables as in the RT, and with Binned_Price as the output variable. Keep
the minimum number of records in a terminal node to 1.

```{r}
bins <- seq(min(car.df$Price),max(car.df$Price),(max(car.df$Price) - min(car.df$Price))/20)
bins
Binned_Price <- .bincode(car.df$Price,bins,include.lowest = TRUE)
Binned_Price <- as.factor(Binned_Price)

```



iCompare the tree generated by the CT with the one generated by the RT. Are
they different? (Look at structure, the top predictors, size of tree, etc.) Why?

```{r}
train.df$Binned_Price <- Binned_Price[train.index]
valid.df$Binned_Price <- Binned_Price[-train.index]
TrainDF_A.binned <- rpart(Binned_Price ~  Age_08_04 + KM + Fuel_Type +HP + Automatic + Doors + Quarterly_Tax +Mfr_Guarantee + Guarantee_Period + Airco+Automatic_airco + CD_Player + Powered_Windows+Sport_Model + Tow_Bar, data = train.df)
prp(TrainDF_A.binned)
t(t(TrainDF_A.binned$variable.importance))

```

iiPredict the price, using the RT and the CT, of a used Toyota Corolla with the
specifications listed in Table 9.6.

```{r}
new.record <- data.frame(Age_08_04 = 77,KM = 117000,Fuel_Type = "Petrol",HP = 110,Automatic = 0,Doors = 5,Quarterly_Tax = 100,Mfr_Guarantee = 0,Guarantee_Period = 3,Airco = 1,Automatic_airco = 0,CD_Player = 0,Powered_Windows = 0,Sport_Model = 0,Tow_Bar = 1)
price.TrainDF_A <- predict(TrainDF_A, newdata = new.record)
price.TrainDF_A.bin <- bins[predict(TrainDF_A.binned, newdata = new.record, type = "class")]
cat(paste("Regression Price Estimate: ",scales::dollar(price.TrainDF_A,0.01)),paste("Classification Price Estimate: ",scales::dollar(price.TrainDF_A.bin,0.01)),sep='\n')
```



iii. Compare the predictions in terms of the predictors that were used, the magnitude of the difference between the two predictions, and the advantages and
disadvantages of the two methods.

Both models are similar and show comparable predictability. Overall the magnitude of the difference was very insignificant, the advantage was that the regression model performed better, and the disadvantage was that the binned model predicted worse.